<!DOCTYPE html>
<html>
<head>
	<title>Portfolio website</title>
	<link rel="stylesheet" type="text/css" href="../css/data_style.css">

	<link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Josefin+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin="anonymous" referrerpolicy="no-referrer" />

</head>
<body>
	<!----hero Section start---->

	<div class="hero">
		<img class="data" src="/img/db.svg"/>
		<nav>
			<h2 class="logo">Project Data</h2>
			<ul>
				<li><a href="../index.html">About Me</a></li>
				<li><a href="introduction.html">Introduction</a></li>
				<li><a href="Code.html">Code</a></li>
				<li><a href="Data.html">Data</a></li>
				<li><a href="DataGathering.html">Data Gathering</a></li>

			</ul>
		
		</nav>

		<div class="content">
			
			<h1>Data</h1>
			<h3>This section focuses on all the data that was collected and the relevance of all the collected information</h3>
            <h4>All data is hosted in this link (insert github link when ready</h4>

			
		</div>
	</div>
	<!----Data collected ---->
	<section class="about">
		<div class="main">
		
			<div class="about-text">
				<h2>What data was used ?</h2>
				<p>Since the main focus of this project is exploring the way music has changed from the 1960s up until now, the core of the work was performed on a huge dataset with thousands of different songs and several numerical and categorical features for these songs (more details later).

                    A major point of relevance this project tries to investigate is music popularity, specifically what are the shifts in popularity trends? When did they happen? Why did they happen? etc. For that reason, the dataset formulated is based on The Billboard Hot 100, which is the music industry standard record chart in the United States for songs, published weekly by Billboard magazine. Chart rankings are based on sales (physical and digital), radio play, and online streaming in the United States. The Billboard Hot 100 has been updated on a weekly basis ever since it was launched on August 4, 1958, resulting in a huge database of popular music.
                    
                    Since this project focuses on the evolution of music by the decade (the 60s compared to the 80s, 90s, etc.) having the weekly Billboard Hot 100’s charts would be redundant and does not serve much-added value. Consequently, the data gathered was the yearly top 100 songs from 1958 and 2015.
                    </p>
		
			</div>
		</div>
	<!-----Data features section start----------->
	<div class="service">
		<div class="title">
			<h2>Data features</h2>
            
		</div>
        <p>The data collected has the following features</p>
		<div class="box">
			<div class="card">
				<i class="fas fa-bars"></i>
				<h5>Lyrics</h5>
				<div class="pra">
					<p>The full lyrics of each song. The repository owners used an API called MusicBrainz API to scrape each song artist's associated genre tags.</p>
					

					
				</div>
			</div>

			<div class="card">
				<i class="far fa-user"></i>
				<h5>Genres</h5>
				<div class="pra">
					<p>Each song is associated with one out of 15 major genres: rock, alternative/indie, electronic/dance, soul, classical/soundtrack, pop,hip-hop/rnb,disco,swing,folk,country,jazz,blues</p>

				
				</div>
			</div>

			<div class="card">
				<i class="far fa-bell"></i>
				<h5>Positivity</h5>
				<div class="pra">
					<p>The overall positivity of a song, scaled between 0 and 1, with 1 being 100% positive. This was gathered by running sentiment analysis on each song’s lyrics using the VADER model on Python </p>

		
				</div>
			</div>

            <div class="card">
				<i class="far fa-bell"></i>
				<h5>Number of words</h5>
				<div class="pra">
					<p>The total number of words in a song </p>

		
				</div>
			</div>


            <div class="card">
				<i class="far fa-bell"></i>
				<h5>Number of lines</h5>
				<div class="pra">
					<p>Number of lines in a song </p>

		
				</div>
			</div>

		</div>
	</div>

    <!-----Second set of features----------->

    <div class="service">


		<div class="box">
			<div class="card">
				<i class="fas fa-bars"></i>
				<h5>Syllables</h5>
				<div class="pra">
					<p>Number of syllables in each song</p>
					

					
				</div>
			</div>

			<div class="card">
				<i class="far fa-user"></i>
				<h5>Difficult words</h5>
				<div class="pra">
					<p>Number of words not on the Dale–Chall "easy" word list.</p>

				
				</div>
			</div>

			<div class="card">
				<i class="far fa-bell"></i>
				<h5>Fog index</h5>
				<div class="pra">
					<p>Gunning-Fog readability index. This index gives the years of formal education required to understand the inputted text</p>

		
				</div>
			</div>

            <div class="card">
				<i class="far fa-bell"></i>
				<h5>Flesch index</h5>
				<div class="pra">
					<p>Flesch reading ease score, which is computed using a variety of linguistics data like average sentence length, word, length, and complexity/number of syllables to determine the readability of a text.   </p>

		
				</div>
			</div>


            <div class="card">
				<i class="far fa-bell"></i>
				<h5>Repetition</h5>
				<div class="pra">
					<p>The number of duplicate lines that appear in the lyrics.</p>

		
				</div>
			</div>

		</div>
	</div>


      <!-----Third set of features----------->

      <div class="service">


		<div class="box">
			<div class="card">
				<i class="fas fa-bars"></i>
				<h5>Acousitcness</h5>
				<div class="pra">
					<p>A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.</p>
					

					
				</div>
			</div>

			<div class="card">
				<i class="far fa-user"></i>
				<h5>URL</h5>
				<div class="pra">
					<p>A URL to access the full audio analysis of this track. An access token is required to access this data.</p>

				
				</div>
			</div>

			<div class="card">
				<i class="far fa-bell"></i>
				<h5>Danceability</h5>
				<div class="pra">
					<p>Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is the least danceable and 1.0 is the most danceable.</p>

		
				</div>
			</div>

            <div class="card">
				<i class="far fa-bell"></i>
				<h5>Duration</h5>
				<div class="pra">
					<p>The duration of the track in milliseconds.</p>
		
				</div>
			</div>


            <div class="card">
				<i class="far fa-bell"></i>
				<h5>Energy</h5>
				<div class="pra">
					<p>Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.</p>

		
				</div>
			</div>

		</div>
	</div>


      <!-----Fourth set of features----------->

      <div class="service">


		<div class="box">
			<div class="card">
				<i class="fas fa-bars"></i>
				<h5>ID</h5>
				<div class="pra">
					<p>Spotify ID for the track</p>
					

					
				</div>
			</div>

			<div class="card">
				<i class="far fa-user"></i>
				<h5>Instrumentalness</h5>
				<div class="pra">
					<p>Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.</p>

				
				</div>
			</div>

			<div class="card">
				<i class="far fa-bell"></i>
				<h5>Key</h5>
				<div class="pra">
					<p>The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.</p>

		
				</div>
			</div>

            <div class="card">
				<i class="far fa-bell"></i>
				<h5>Liveness</h5>
				<div class="pra">
					<p>Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides a strong likelihood that the track is live.

                    </p>
		
				</div>
			</div>


            <div class="card">
				<i class="far fa-bell"></i>
				<h5>Loudness</h5>
				<div class="pra">

                    <p>The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing the relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.</p>

		
				</div>
			</div>

		</div>
	</div>
   <!-----Fifth set of features----------->

   <div class="service">


    <div class="box">
        <div class="card">
            <i class="fas fa-bars"></i>
            <h5>Mode</h5>
            <div class="pra">
                <p>Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.</p>
                

                
            </div>
        </div>

        <div class="card">
            <i class="far fa-user"></i>
            <h5>Speechiness</h5>
            <div class="pra">
                <p>Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.</p>

            
            </div>
        </div>

        <div class="card">
            <i class="far fa-bell"></i>
            <h5>Tempo</h5>
            <div class="pra">
                <p>The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, the tempo is the speed or pace of a given piece and derives directly from the average beat duration.</p>

    
            </div>
        </div>

        <div class="card">
            <i class="far fa-bell"></i>
            <h5>Time signature</h5>
            <div class="pra">
                <p>An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of "3/4", to "7/4".

                </p>
    
            </div>
        </div>


        <div class="card">
            <i class="far fa-bell"></i>
            <h5>Valence</h5>
            <div class="pra">

                <p>A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).
                   </p>
                    

    
            </div>
        </div>

    </div>
</div>











</body>
</html>